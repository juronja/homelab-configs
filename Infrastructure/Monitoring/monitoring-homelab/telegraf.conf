# Docs: https://github.com/influxdata/telegraf/tree/master/plugins

# Telegraf Configuration
#
# Telegraf is entirely plugin driven. All metrics are gathered from the
# declared inputs, and sent to the declared outputs.
#
# Plugins must be declared in here to be active.
# To deactivate a plugin, comment out the name and any variables.
#
# Use 'telegraf -config telegraf.conf -test' to see what metrics a config
# file would generate.
#
# Environment variables can be used anywhere in this config file, simply surround
# them with ${}. For strings the variable must be within quotes (ie, "${STR_VAR}"),
# for numbers and booleans they should be plain (ie, ${INT_VAR}, ${BOOL_VAR})


# Global tags can be specified here in key="value" format.
[global_tags]
  # dc = "us-east-1" # will tag all metrics with dc=us-east-1
  # rack = "1a"
  ## Environment variables can be used as tags, and throughout the config file
  # user = "$USER"

# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "10s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## Maximum number of unwritten metrics per output.  Increasing this value
  ## allows for longer periods of output downtime without dropping metrics at the
  ## cost of higher maximum memory usage.
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Collection offset is used to shift the collection by the given amount.
  ## This can be be used to avoid many plugins querying constraint devices
  ## at the same time by manually scheduling them in time.
  # collection_offset = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## Collected metrics are rounded to the precision specified. Precision is
  ## specified as an interval with an integer + unit (e.g. 0s, 10ms, 2us, 4s).
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  ##
  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s:
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ##
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  precision = "0s"

  # # Log at debug level.
  # debug = false
  # # Log only error level messages.
  # quiet = false

  ## Log format controls the way messages are logged and can be one of "text",
  ## "structured" or, on Windows, "eventlog".
  # logformat = "text"

  ## Message key for structured logs, to override the default of "msg".
  ## Ignored if `logformat` is not "structured".
  # structured_log_message_key = "message"

  ## Name of the file to be logged to or stderr if unset or empty. This
  ## setting is ignored for the "eventlog" format.
  # logfile = ""

  ## The logfile will be rotated after the time interval specified.  When set
  ## to 0 no time based rotation is performed.  Logs are rotated only when
  ## written to, if there is no log activity rotation may be delayed.
  # logfile_rotation_interval = "0h"

  ## The logfile will be rotated when it becomes larger than the specified
  ## size.  When set to 0 no size based rotation is performed.
  # logfile_rotation_max_size = "0MB"

  ## Maximum number of rotated archives to keep, any older logs are deleted.
  ## If set to -1, no archives are removed.
  # logfile_rotation_max_archives = 5

  ## Pick a timezone to use when logging or type 'local' for local time.
  ## Example: America/Chicago
  # log_with_timezone = ""

  # Override default hostname, if empty use os.Hostname()
  hostname = "telegraf"
  # If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false

  ## Method of translating SNMP objects. Can be "netsnmp" (deprecated) which
  ## translates by calling external programs snmptranslate and snmptable,
  ## or "gosmi" which translates using the built-in gosmi library.
  # snmp_translator = "netsnmp"

  ## Name of the file to load the state of plugins from and store the state to.
  ## If uncommented and not empty, this file will be used to save the state of
  ## stateful plugins on termination of Telegraf. If the file exists on start,
  ## the state in the file will be restored for the plugins.
  # statefile = ""

  ## Flag to skip running processors after aggregators
  ## By default, processors are run a second time after aggregators. Changing
  ## this setting to true will skip the second run of processors.
  # skip_processors_after_aggregators = false

###############################################################################
#                            SECRETSTORE PLUGINS                              #
###############################################################################


# # Secret-store to access Docker Secrets
# [[secretstores.docker]]
#   ## Unique identifier for the secretstore.
#   ## This id can later be used in plugins to reference the secrets
#   ## in this secret-store via @{<id>:<secret_key>} (mandatory)
#   id = "docker_secretstore"
#
#   ## Default Path to directory where docker stores the secrets file
#   ## Current implementation in docker compose v2 only allows the following
#   ## value for the path where the secrets are mounted at runtime
#   # path = "/run/secrets"
#
#   ## Allow dynamic secrets that are updated during runtime of telegraf
#   ## Dynamic Secrets work only with `file` or `external` configuration
#   ## in `secrets` section of the `docker-compose.yml` file
#   # dynamic = false


# # Read secrets from a HTTP endpoint
# [[secretstores.http]]
#   ## Unique identifier for the secret-store.
#   ## This id can later be used in plugins to reference the secrets
#   ## in this secret-store via @{<id>:<secret_key>} (mandatory)
#   id = "secretstore"
#
#   ## URLs from which to read the secrets
#   url = "http://localhost/secrets"
#
#   ## Optional HTTP headers
#   # headers = {"X-Special-Header" = "Special-Value"}
#
#   ## Optional Token for Bearer Authentication via
#   ## "Authorization: Bearer <token>" header
#   # token = "your-token"
#
#   ## Optional Credentials for HTTP Basic Authentication
#   # username = "username"
#   # password = "pa$$word"
#
#   ## OAuth2 Client Credentials. The options 'client_id', 'client_secret', and 'token_url' are required to use OAuth2.
#   # client_id = "clientid"
#   # client_secret = "secret"
#   # token_url = "https://indentityprovider/oauth2/v1/token"
#   # scopes = ["urn:opc:idm:__myscopes__"]
#
#   ## HTTP Proxy support
#   # use_system_proxy = false
#   # http_proxy_url = ""
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Minimal TLS version to accept by the client
#   # tls_min_version = "TLS12"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Optional Cookie authentication
#   # cookie_auth_url = "https://localhost/authMe"
#   # cookie_auth_method = "POST"
#   # cookie_auth_username = "username"
#   # cookie_auth_password = "pa$$word"
#   # cookie_auth_headers = { Content-Type = "application/json", X-MY-HEADER = "hello" }
#   # cookie_auth_body = '{"username": "user", "password": "pa$$word", "authenticate": "me"}'
#   ## When unset or set to zero the authentication will only happen once
#   ## and will never renew the cookie. Set to a suitable duration if you
#   ## require cookie renewal!
#   # cookie_auth_renewal = "0s"
#
#   ## Amount of time allowed to complete the HTTP request
#   # timeout = "5s"
#
#   ## List of success status codes
#   # success_status_codes = [200]
#
#   ## JSONata expression to transform the server response into a
#   ##   { "secret name": "secret value", ... }
#   ## form. See https://jsonata.org for more information and a playground.
#   # transformation = ''
#
#   ## Cipher used to decrypt the secrets.
#   ## In case your secrets are transmitted in an encrypted form, you need
#   ## to specify the cipher used and provide the corresponding configuration.
#   ## Please refer to https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/http/README.md
#   ## for supported values.
#   # cipher = "none"
#
#   ## AES cipher parameters
#   # [secretstores.http.aes]
#   #   ## Key (hex-encoded) and initialization-vector (IV) for the decryption.
#   #   ## In case the key (and IV) is derived from a password, the values can
#   #   ## be omitted.
#   #   key = ""
#   #   init_vector = ""
#   #
#   #   ## Parameters for password-based-key derivation.
#   #   ## These parameters must match the encryption side to derive the same
#   #   ## key on both sides!
#   #   # kdf_algorithm = "PBKDF2-HMAC-SHA256"
#   #   # password = ""
#   #   # salt = ""
#   #   # iterations = 0


# # File based Javascript Object Signing and Encryption based secret-store
# [[secretstores.jose]]
#   ## Unique identifier for the secret-store.
#   ## This id can later be used in plugins to reference the secrets
#   ## in this secret-store via @{<id>:<secret_key>} (mandatory)
#   id = "secretstore"
#
#   ## Directory for storing the secrets
#   path = "/etc/telegraf/secrets"
#
#   ## Password to access the secrets.
#   ## If no password is specified here, Telegraf will prompt for it at startup time.
#   # password = ""


# # Secret-store to retrieve and maintain tokens from various OAuth2 services
# [[secretstores.oauth2]]
#   ## Unique identifier for the secret-store.
#   ## This id can later be used in plugins to reference the secrets
#   ## in this secret-store via @{<id>:<secret_key>} (mandatory)
#   id = "secretstore"
#
#   ## Service to retrieve the token(s) from
#   ## Currently supported services are "custom", "auth0" and "AzureAD"
#   # service = "custom"
#
#   ## Setting to overwrite the queried token-endpoint
#   ## This setting is optional for some services but mandatory for others such
#   ## as "custom" or "auth0". Please check the documentation at
#   ## https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/oauth2/README.md
#   # token_endpoint = ""
#
#   ## Tenant ID for the AzureAD service
#   # tenant_id = ""
#
#   ## Minimal remaining time until the token expires
#   ## If a token expires less than the set duration in the future, the token is
#   ## renewed. This is useful to avoid race-condition issues where a token is
#   ## still valid, but isn't when the request reaches the API endpoint of
#   ## your service using the token.
#   # token_expiry_margin = "1s"
#
#   ## Section for defining a token secret
#   [[secretstores.oauth2.token]]
#     ## Unique secret-key used for referencing the token via @{<id>:<secret_key>}
#     key = ""
#     ## Client-ID and secret for the 2-legged OAuth flow
#     client_id = ""
#     client_secret = ""
#     ## Scopes to send in the request
#     # scopes = []
#
#     ## Additional (optional) parameters to include in the token request
#     ## This might for example include the "audience" parameter required for
#     ## auth0.
#     # [secretstores.oauth2.token.parameters]
#     #     audience = ""


# # Operating System native secret-store
# [[secretstores.os]]
#   ## Unique identifier for the secret-store.
#   ## This id can later be used in plugins to reference the secrets
#   ## in this secret-store via @{<id>:<secret_key>} (mandatory)
#   id = "secretstore"
#
#   ## Keyring Name & Collection
#   ## * Linux: keyring name used for the secrets, collection is unused
#   ## * macOS: keyring specifies the macOS' Keychain name and collection is an
#   ##     optional Keychain service name
#   ## * Windows: keys follow a fixed pattern in the form
#   ##     `<collection>:<keyring>:<key_name>`. Please keep this in mind when
#   ##     creating secrets with the Windows credential tool.
#   # keyring = "telegraf"
#   # collection = ""
#
#   ## macOS Keychain password
#   ## If no password is specified here, Telegraf will prompt for it at startup
#   ## time.
#   # password = ""
#
#   ## Allow dynamic secrets that are updated during runtime of telegraf
#   # dynamic = false


# # Secret-store to access systemd secrets
# [[secretstores.systemd]]
#   ## Unique identifier for the secretstore.
#   ## This id can later be used in plugins to reference the secrets
#   ## in this secret-store via @{<id>:<secret_key>} (mandatory)
#   id = "systemd"
#
#   ## Path to systemd credentials directory
#   ## This should not be required as systemd indicates this directory
#   ## via the CREDENTIALS_DIRECTORY environment variable.
#   # path = "${CREDENTIALS_DIRECTORY}"
#
#   ## Prefix to remove from systemd credential-filenames to derive secret names
#   # prefix = "telegraf."
#


###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################


# # Configuration for sending metrics to InfluxDB
# [[outputs.influxdb]]
#   ## The full HTTP or UDP URL for your InfluxDB instance.
#   ##
#   ## Multiple URLs can be specified for a single cluster, only ONE of the
#   ## urls will be written to each interval.
#   # urls = ["unix:///var/run/influxdb.sock"]
#   # urls = ["udp://127.0.0.1:8089"]
#   # urls = ["http://127.0.0.1:8086"]
#
#   ## Local address to bind when connecting to the server
#   ## If empty or not set, the local address is automatically chosen.
#   # local_address = ""
#
#   ## The target database for metrics; will be created as needed.
#   ## For UDP url endpoint database needs to be configured on server side.
#   # database = "telegraf"
#
#   ## The value of this tag will be used to determine the database.  If this
#   ## tag is not set the 'database' option is used as the default.
#   # database_tag = ""
#
#   ## If true, the 'database_tag' will not be included in the written metric.
#   # exclude_database_tag = false
#
#   ## If true, no CREATE DATABASE queries will be sent.  Set to true when using
#   ## Telegraf with a user without permissions to create databases or when the
#   ## database already exists.
#   # skip_database_creation = false
#
#   ## Name of existing retention policy to write to.  Empty string writes to
#   ## the default retention policy.  Only takes effect when using HTTP.
#   # retention_policy = ""
#
#   ## The value of this tag will be used to determine the retention policy.  If this
#   ## tag is not set the 'retention_policy' option is used as the default.
#   # retention_policy_tag = ""
#
#   ## If true, the 'retention_policy_tag' will not be included in the written metric.
#   # exclude_retention_policy_tag = false
#
#   ## Write consistency (clusters only), can be: "any", "one", "quorum", "all".
#   ## Only takes effect when using HTTP.
#   # write_consistency = "any"
#
#   ## Timeout for HTTP messages.
#   # timeout = "5s"
#
#   ## HTTP Basic Auth
#   # username = "telegraf"
#   # password = "metricsmetricsmetricsmetrics"
#
#   ## HTTP User-Agent
#   # user_agent = "telegraf"
#
#   ## UDP payload size is the maximum packet size to send.
#   # udp_payload = "512B"
#
#   ## Optional TLS Config for use on HTTP connections.
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## HTTP Proxy override, if unset values the standard proxy environment
#   ## variables are consulted to determine which proxy, if any, should be used.
#   # http_proxy = "http://corporate.proxy:3128"
#
#   ## Additional HTTP headers
#   # http_headers = {"X-Special-Header" = "Special-Value"}
#
#   ## HTTP Content-Encoding for write request body, can be set to "gzip" to
#   ## compress body or "identity" to apply no encoding.
#   # content_encoding = "gzip"
#
#   ## When true, Telegraf will output unsigned integers as unsigned values,
#   ## i.e.: "42u".  You will need a version of InfluxDB supporting unsigned
#   ## integer values.  Enabling this option will result in field type errors if
#   ## existing data has been written.
#   # influx_uint_support = false
#
#   ## When true, Telegraf will omit the timestamp on data to allow InfluxDB
#   ## to set the timestamp of the data during ingestion. This is generally NOT
#   ## what you want as it can lead to data points captured at different times
#   ## getting omitted due to similar data.
#   # influx_omit_timestamp = false


# Configuration for the Prometheus client to spawn
[[outputs.prometheus_client]]
  ## Address to listen on.
  ##   ex:
  ##     listen = ":9273"
  listen = ":9273"

  ## Maximum duration before timing out read of the request
  # read_timeout = "10s"
  ## Maximum duration before timing out write of the response
  # write_timeout = "10s"

  ## Metric version controls the mapping from Prometheus metrics into Telegraf metrics.
  ## See "Metric Format Configuration" in plugins/inputs/prometheus/README.md for details.
  ## Valid options: 1, 2
  # metric_version = 1

  ## Use HTTP Basic Authentication.
  # basic_username = "Foo"
  # basic_password = "Bar"

  ## If set, the IP Ranges which are allowed to access metrics.
  ##   ex: ip_range = ["192.168.0.0/24", "192.168.1.0/30"]
  # ip_range = []

  ## Path to publish the metrics on.
  # path = "/metrics"

  ## Expiration interval for each metric. 0 == no expiration
  # expiration_interval = "60s"

  ## Collectors to enable, valid entries are "gocollector" and "process".
  ## If unset, both are enabled.
  # collectors_exclude = ["gocollector", "process"]

  ## Send string metrics as Prometheus labels.
  ## Unless set to false all string metrics will be sent as labels.
  # string_as_label = true

  ## If set, enable TLS with the given certificate.
  # tls_cert = "/etc/ssl/telegraf.crt"
  # tls_key = "/etc/ssl/telegraf.key"

  ## Set one or more allowed client CA certificate file names to
  ## enable mutually authenticated TLS connections
  # tls_allowed_cacerts = ["/etc/telegraf/clientca.pem"]

  ## Export metric collection time.
  # export_timestamp = false

  ## Set custom headers for HTTP responses.
  # http_headers = {"X-Special-Header" = "Special-Value"}

  ## Specify the metric type explicitly.
  ## This overrides the metric-type of the Telegraf metric. Globbing is allowed.
  # [outputs.prometheus_client.metric_types]
  #   counter = []
  #   gauge = []


###############################################################################
#                            PROCESSOR PLUGINS                                #
###############################################################################


# # Apply metric modifications using override semantics.
# [[processors.override]]
#   ## All modifications on inputs and aggregators can be overridden:
#   # name_override = "new_name"
#   # name_prefix = "new_name_prefix"
#   # name_suffix = "new_name_suffix"
#
#   ## Tags to be added (all values must be strings)
#   # [processors.override.tags]
#   #   additional_tag = "tag_value"


###############################################################################
#                            AGGREGATOR PLUGINS                               #
###############################################################################


# # Keep the aggregate basicstats of each metric passing through.
# [[aggregators.basicstats]]
#   ## The period on which to flush & clear the aggregator.
#   # period = "30s"
#
#   ## If true, the original metric will be dropped by the
#   ## aggregator and will not get sent to the output plugins.
#   # drop_original = false
#
#   ## Configures which basic stats to push as fields
#   # stats = ["count","min","max","mean","variance","stdev"]


###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################


# # Read metrics about cpu usage
# [[inputs.cpu]]
#   ## Whether to report per-cpu stats or not
#   percpu = true
#   ## Whether to report total system cpu stats or not
#   totalcpu = true
#   ## If true, collect raw CPU time metrics
#   collect_cpu_time = false
#   ## If true, compute and report the sum of all non-idle CPU states
#   ## NOTE: The resulting 'time_active' field INCLUDES 'iowait'!
#   report_active = false
#   ## If true and the info is available then add core_id and physical_id tags
#   core_tags = false


# # Read metrics about disk usage by mount point
# [[inputs.disk]]
#   ## By default stats will be gathered for all mount points.
#   ## Set mount_points will restrict the stats to only the specified mount points.
#   # mount_points = ["/"]

#   ## Ignore mount points by filesystem type.
#   ignore_fs = ["tmpfs", "devtmpfs", "devfs", "iso9660", "overlay", "aufs", "squashfs"]

#   ## Ignore mount points by mount options.
#   ## The 'mount' command reports options of all mounts in parathesis.
#   ## Bind mounts can be ignored with the special 'bind' option.
#   # ignore_mount_opts = []


# # Read metrics about disk IO by device
# [[inputs.diskio]]
#   ## Devices to collect stats for
#   ## Wildcards are supported except for disk synonyms like '/dev/disk/by-id'.
#   ## ex. devices = ["sda", "sdb", "vd*", "/dev/disk/by-id/nvme-eui.00123deadc0de123"]
#   # devices = ["*"]

#   ## Skip gathering of the disk's serial numbers.
#   # skip_serial_number = true

#   ## Device metadata tags to add on systems supporting it (Linux only)
#   ## Use 'udevadm info -q property -n <device>' to get a list of properties.
#   ## Note: Most, but not all, udev properties can be accessed this way. Properties
#   ## that are currently inaccessible include DEVTYPE, DEVNAME, and DEVPATH.
#   # device_tags = ["ID_FS_TYPE", "ID_FS_USAGE"]

#   ## Using the same metadata source as device_tags, you can also customize the
#   ## name of the device via templates.
#   ## The 'name_templates' parameter is a list of templates to try and apply to
#   ## the device. The template may contain variables in the form of '$PROPERTY' or
#   ## '${PROPERTY}'. The first template which does not contain any variables not
#   ## present for the device is used as the device name tag.
#   ## The typical use case is for LVM volumes, to get the VG/LV name instead of
#   ## the near-meaningless DM-0 name.
#   # name_templates = ["$ID_FS_LABEL","$DM_VG_NAME/$DM_LV_NAME"]


# # Plugin to collect various Linux kernel statistics.
# # This plugin ONLY supports Linux
# [[inputs.kernel]]
#   ## Additional gather options
#   ## Possible options include:
#   ## * ksm - kernel same-page merging
#   ## * psi - pressure stall information
#   # collect = []


# # Read metrics about memory usage
# [[inputs.mem]]
#   # no configuration


# # Get the number of processes and group them by status
# # This plugin ONLY supports non-Windows
# [[inputs.processes]]
#   ## Use sudo to run ps command on *BSD systems. Linux systems will read
#   ## /proc, so this does not apply there.
#   # use_sudo = false


# # Read metrics about swap memory usage
# [[inputs.swap]]
#   # no configuration


# # Read metrics about system load & uptime
# [[inputs.system]]
#   # no configuration


# Read metrics about docker containers
[[inputs.docker]]
  ## Docker Endpoint
  ##   To use TCP, set endpoint = "tcp://[ip]:[port]"
  ##   To use environment variables (ie, docker-machine), set endpoint = "ENV"
  endpoint = "unix:///var/run/docker.sock"

  ## Set to true to collect Swarm metrics(desired_replicas, running_replicas)
  ## Note: configure this in one of the manager nodes in a Swarm cluster.
  ## configuring in multiple Swarm managers results in duplication of metrics.
  gather_services = false

  ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars
  source_tag = false

  ## Containers to include and exclude. Collect all if empty. Globs accepted.
  container_name_include = []
  container_name_exclude = []

  ## Container states to include and exclude. Globs accepted.
  ## When empty only containers in the "running" state will be captured.
  ## example: container_state_include = ["created", "restarting", "running", "removing", "paused", "exited", "dead"]
  ## example: container_state_exclude = ["created", "restarting", "running", "removing", "paused", "exited", "dead"]
  # container_state_include = []
  # container_state_exclude = []

  ## Objects to include for disk usage query
  ## Allowed values are "container", "image", "volume"
  ## When empty disk usage is excluded
  storage_objects = ["container"]

  ## Timeout for docker list, info, and stats commands
  timeout = "5s"

  ## Specifies for which classes a per-device metric should be issued
  ## Possible values are 'cpu' (cpu0, cpu1, ...), 'blkio' (8:0, 8:1, ...) and 'network' (eth0, eth1, ...)
  # perdevice_include = ["cpu"]

  ## Specifies for which classes a total metric should be issued. Total is an aggregated of the 'perdevice_include' values.
  ## Possible values are 'cpu', 'blkio' and 'network'
  ## Total 'cpu' is reported directly by Docker daemon, and 'network' and 'blkio' totals are aggregated by this plugin.
  # total_include = ["cpu", "blkio", "network"]

  ## docker labels to include and exclude as tags.  Globs accepted.
  ## Note that an empty array for both will include all labels as tags
  docker_label_include = []
  docker_label_exclude = []

  ## Which environment variables should we use as a tag
  tag_env = ["JAVA_HOME", "HEAP_SIZE"]

  ## Optional TLS Config
  # tls_ca = "/etc/telegraf/ca.pem"
  # tls_cert = "/etc/telegraf/cert.pem"
  # tls_key = "/etc/telegraf/key.pem"
  ## Use TLS but skip chain & host verification
  # insecure_skip_verify = false


# # HTTP/HTTPS request given an address a method and a timeout
# [[inputs.http_response]]
#   ## List of urls to query.
#   # urls = ["http://localhost"]
#
#   ## Set http_proxy.
#   ## Telegraf uses the system wide proxy settings if it's is not set.
#   # http_proxy = "http://localhost:8888"
#
#   ## Set response_timeout (default 5 seconds)
#   # response_timeout = "5s"
#
#   ## HTTP Request Method
#   # method = "GET"
#
#   ## Whether to follow redirects from the server (defaults to false)
#   # follow_redirects = false
#
#   ## Optional file with Bearer token
#   ## file content is added as an Authorization header
#   # bearer_token = "/path/to/file"
#
#   ## Optional HTTP Basic Auth Credentials
#   # username = "username"
#   # password = "pa$$word"
#
#   ## Optional HTTP Request Body
#   # body = '''
#   # {'fake':'data'}
#   # '''
#
#   ## Optional HTTP Request Body Form
#   ## Key value pairs to encode and set at URL form. Can be used with the POST
#   ## method + application/x-www-form-urlencoded content type to replicate the
#   ## POSTFORM method.
#   # body_form = { "key": "value" }
#
#   ## Optional name of the field that will contain the body of the response.
#   ## By default it is set to an empty String indicating that the body's
#   ## content won't be added
#   # response_body_field = ''
#
#   ## Maximum allowed HTTP response body size in bytes.
#   ## 0 means to use the default of 32MiB.
#   ## If the response body size exceeds this limit a "body_read_error" will
#   ## be raised.
#   # response_body_max_size = "32MiB"
#
#   ## Optional substring or regex match in body of the response (case sensitive)
#   # response_string_match = "\"service_status\": \"up\""
#   # response_string_match = "ok"
#   # response_string_match = "\".*_status\".?:.?\"up\""
#
#   ## Expected response status code.
#   ## The status code of the response is compared to this value. If they match,
#   ## the field "response_status_code_match" will be 1, otherwise it will be 0.
#   ## If the expected status code is 0, the check is disabled and the field
#   ## won't be added.
#   # response_status_code = 0
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#   ## Use the given name as the SNI server name on each URL
#   # tls_server_name = ""
#   ## TLS renegotiation method, choose from "never", "once", "freely"
#   # tls_renegotiation_method = "never"
#
#   ## HTTP Request Headers (all values must be strings)
#   # [inputs.http_response.headers]
#   #   Host = "github.com"
#
#   ## Optional setting to map response http headers into tags
#   ## If the http header is not present on the request, no corresponding tag will
#   ## be added. If multiple instances of the http header are present, only the
#   ## first value will be used.
#   # http_header_tags = {"HTTP_HEADER" = "TAG_NAME"}
#
#   ## Interface to use when dialing an address
#   # interface = "eth0"
#
#   ## Optional Cookie authentication
#   # cookie_auth_url = "https://localhost/authMe"
#   # cookie_auth_method = "POST"
#   # cookie_auth_username = "username"
#   # cookie_auth_password = "pa$$word"
#   # cookie_auth_body = '{"username": "user", "password": "pa$$word", "authenticate": "me"}'
#   ## cookie_auth_renewal not set or set to "0" will auth once and never renew the cookie
#   # cookie_auth_renewal = "5m"


# # Read jobs and cluster metrics from Jenkins instances
# [[inputs.jenkins]]
#   ## The Jenkins URL in the format "schema://host:port"
#   url = "http://my-jenkins-instance:8080"
#   # username = "admin"
#   # password = "admin"
#
#   ## Set response_timeout
#   response_timeout = "5s"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use SSL but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Optional Max Job Build Age filter
#   ## Default 1 hour, ignore builds older than max_build_age
#   # max_build_age = "1h"
#
#   ## Optional Sub Job Depth filter
#   ## Jenkins can have unlimited layer of sub jobs
#   ## This config will limit the layers of pulling, default value 0 means
#   ## unlimited pulling until no more sub jobs
#   # max_subjob_depth = 0
#
#   ## Optional Sub Job Per Layer
#   ## In workflow-multibranch-plugin, each branch will be created as a sub job.
#   ## This config will limit to call only the lasted branches in each layer,
#   ## empty will use default value 10
#   # max_subjob_per_layer = 10
#
#   ## Jobs to include or exclude from gathering
#   ## When using both lists, job_exclude has priority.
#   ## Wildcards are supported: [ "jobA/*", "jobB/subjob1/*"]
#   # job_include = [ "*" ]
#   # job_exclude = [ ]
#
#   ## Nodes to include or exclude from gathering
#   ## When using both lists, node_exclude has priority.
#   # node_include = [ "*" ]
#   # node_exclude = [ ]
#
#   ## Worker pool for jenkins plugin only
#   ## Empty this field will use default value 5
#   # max_connections = 5
#
#   ## When set to true will add node labels as a comma-separated tag. If none,
#   ## are found, then a tag with the value of 'none' is used. Finally, if a
#   ## label contains a comma it is replaced with an underscore.
#   # node_labels_as_tag = false


# # Read metrics from the Kubernetes api
# [[inputs.kube_inventory]]
#   ## URL for the Kubernetes API.
#   ## If empty in-cluster config with POD's service account token will be used.
#   # url = ""
#
#   ## URL for the kubelet, if set it will be used to collect the pods resource metrics
#   # url_kubelet = "http://127.0.0.1:10255"
#
#   ## Namespace to use. Set to "" to use all namespaces.
#   # namespace = "default"
#
#   ## Node name to filter to. No filtering by default.
#   # node_name = ""
#
#   ## Use bearer token for authorization.
#   ## Ignored if url is empty and in-cluster config is used.
#   # bearer_token = "/var/run/secrets/kubernetes.io/serviceaccount/token"
#
#   ## Set response_timeout (default 5 seconds)
#   # response_timeout = "5s"
#
#   ## Optional Resources to exclude from gathering
#   ## Leave them with blank with try to gather everything available.
#   ## Values can be - "daemonsets", deployments", "endpoints", "ingress",
#   ## "nodes", "persistentvolumes", "persistentvolumeclaims", "pods", "services",
#   ## "statefulsets"
#   # resource_exclude = [ "deployments", "nodes", "statefulsets" ]
#
#   ## Optional Resources to include when gathering
#   ## Overrides resource_exclude if both set.
#   # resource_include = [ "deployments", "nodes", "statefulsets" ]
#
#   ## selectors to include and exclude as tags.  Globs accepted.
#   ## Note that an empty array for both will include all selectors as tags
#   ## selector_exclude overrides selector_include if both set.
#   # selector_include = []
#   # selector_exclude = ["*"]
#
#   ## Optional TLS Config
#   ## Trusted root certificates for server
#   # tls_ca = "/path/to/cafile"
#   ## Used for TLS client certificate authentication
#   # tls_cert = "/path/to/certfile"
#   ## Used for TLS client certificate authentication
#   # tls_key = "/path/to/keyfile"
#   ## Send the specified TLS server name via SNI
#   # tls_server_name = "kubernetes.example.com"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Uncomment to remove deprecated metrics.
#   # fieldexclude = ["terminated_reason"]


# # Read metrics from the kubernetes kubelet api
# [[inputs.kubernetes]]
#   ## URL for the kubelet, if empty read metrics from all nodes in the cluster
#   url = "http://127.0.0.1:10255"
#
#   ## Use bearer token for authorization. ('bearer_token' takes priority)
#   ## If both of these are empty, we'll use the default serviceaccount:
#   ## at: /var/run/secrets/kubernetes.io/serviceaccount/token
#   ##
#   ## To re-read the token at each interval, please use a file with the
#   ## bearer_token option. If given a string, Telegraf will always use that
#   ## token.
#   # bearer_token = "/var/run/secrets/kubernetes.io/serviceaccount/token"
#   ## OR
#   # bearer_token_string = "abc_123"
#
#   ## Kubernetes Node Metric Name
#   ## The default Kubernetes node metric name (i.e. kubernetes_node) is the same
#   ## for the kubernetes and kube_inventory plugins. To avoid conflicts, set this
#   ## option to a different value.
#   # node_metric_name = "kubernetes_node"
#
#   ## Pod labels to be added as tags.  An empty array for both include and
#   ## exclude will include all labels.
#   # label_include = []
#   # label_exclude = ["*"]
#
#   ## Set response_timeout (default 5 seconds)
#   # response_timeout = "5s"
#
#   ## Optional TLS Config
#   # tls_ca = /path/to/cafile
#   # tls_cert = /path/to/certfile
#   # tls_key = /path/to/keyfile
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false


# # Ping given url(s) and return statistics
# [[inputs.ping]]
#   ## Hosts to send ping packets to.
#   urls = ["example.org"]
#
#   ## Method used for sending pings, can be either "exec" or "native".  When set
#   ## to "exec" the systems ping command will be executed.  When set to "native"
#   ## the plugin will send pings directly.
#   ##
#   ## While the default is "exec" for backwards compatibility, new deployments
#   ## are encouraged to use the "native" method for improved compatibility and
#   ## performance.
#   # method = "exec"
#
#   ## Number of ping packets to send per interval.  Corresponds to the "-c"
#   ## option of the ping command.
#   # count = 1
#
#   ## Time to wait between sending ping packets in seconds.  Operates like the
#   ## "-i" option of the ping command.
#   # ping_interval = 1.0
#
#   ## If set, the time to wait for a ping response in seconds.  Operates like
#   ## the "-W" option of the ping command.
#   # timeout = 1.0
#
#   ## If set, the total ping deadline, in seconds.  Operates like the -w option
#   ## of the ping command.
#   # deadline = 10
#
#   ## Interface or source address to send ping from.  Operates like the -I or -S
#   ## option of the ping command.
#   # interface = ""
#
#   ## Percentiles to calculate. This only works with the native method.
#   # percentiles = [50, 95, 99]
#
#   ## Specify the ping executable binary.
#   # binary = "ping"
#
#   ## Arguments for ping command. When arguments is not empty, the command from
#   ## the binary option will be used and other options (ping_interval, timeout,
#   ## etc) will be ignored.
#   # arguments = ["-c", "3"]
#
#   ## Use only IPv4 addresses when resolving a hostname. By default, both IPv4
#   ## and IPv6 can be used.
#   # ipv4 = false
#
#   ## Use only IPv6 addresses when resolving a hostname. By default, both IPv4
#   ## and IPv6 can be used.
#   # ipv6 = false
#
#   ## Number of data bytes to be sent. Corresponds to the "-s"
#   ## option of the ping command. This only works with the native method.
#   # size = 56


# # Provides metrics from Proxmox nodes (Proxmox Virtual Environment > 6.2).
# [[inputs.proxmox]]
#   ## API connection configuration. The API token was introduced in Proxmox v6.2.
#   ## Required permissions for user and token: PVEAuditor role on /.
#   base_url = "https://pve-IP:8006/api2/json"
#   api_token = "metrics@pam!telegraf=API-KEY"

#   ## Node name, defaults to OS hostname
#   ## Unless Telegraf is on the same host as Proxmox, setting this is required.
#   node_name = "pve-NAME"

#   ## Additional tags of the VM stats data to add as a tag
#   ## Supported values are "vmid" and "status"
#   # additional_vmstats_tags = []

#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false

#   ## HTTP response timeout (default: 5s)
#   # response_timeout = "5s"


# # Read metrics from storage devices supporting S.M.A.R.T.
# [[inputs.smart]]
#     ## Optionally specify the path to the smartctl executable
#     # path_smartctl = "/usr/bin/smartctl"
#
#     ## Optionally specify the path to the nvme-cli executable
#     # path_nvme = "/usr/bin/nvme"
#
#     ## Optionally specify if vendor specific attributes should be propagated for NVMe disk case
#     ## ["auto-on"] - automatically find and enable additional vendor specific disk info
#     ## ["vendor1", "vendor2", ...] - e.g. "Intel" enable additional Intel specific disk info
#     # enable_extensions = ["auto-on"]
#
#     ## On most platforms used cli utilities requires root access.
#     ## Setting 'use_sudo' to true will make use of sudo to run smartctl or nvme-cli.
#     ## Sudo must be configured to allow the telegraf user to run smartctl or nvme-cli
#     ## without a password.
#     # use_sudo = false
#
#     ## Adds an extra tag "device_type", which can be used to differentiate
#     ## multiple disks behind the same controller (e.g., MegaRAID).
#     # tag_with_device_type = false
#
#     ## Skip checking disks in this power mode. Defaults to
#     ## "standby" to not wake up disks that have stopped rotating.
#     ## See --nocheck in the man pages for smartctl.
#     ## smartctl version 5.41 and 5.42 have faulty detection of
#     ## power mode and might require changing this value to
#     ## "never" depending on your disks.
#     # nocheck = "standby"
#
#     ## Gather all returned S.M.A.R.T. attribute metrics and the detailed
#     ## information from each drive into the 'smart_attribute' measurement.
#     # attributes = false
#
#     ## Optionally specify devices to exclude from reporting if disks auto-discovery is performed.
#     # excludes = [ "/dev/pass6" ]
#
#     ## Optionally specify devices and device type, if unset
#     ## a scan (smartctl --scan and smartctl --scan -d nvme) for S.M.A.R.T. devices will be done
#     ## and all found will be included except for the excluded in excludes.
#     # devices = [ "/dev/ada0 -d atacam", "/dev/nvme0"]
#
#     ## Timeout for the cli command to complete.
#     # timeout = "30s"
#
#     ## Optionally call smartctl and nvme-cli with a specific concurrency policy.
#     ## By default, smartctl and nvme-cli are called in separate threads (goroutines) to gather disk attributes.
#     ## Some devices (e.g. disks in RAID arrays) may have access limitations that require sequential reading of
#     ## SMART data - one individual array drive at the time. In such case please set this configuration option
#     ## to "sequential" to get readings for all drives.
#     ## valid options: concurrent, sequential
#     # read_method = "concurrent"


# # Read metrics from SMART storage devices using smartclt's JSON output
# [[inputs.smartctl]]
#     ## Optionally specify the path to the smartctl executable
#     # path = "/usr/sbin/smartctl"
#
#     ## Use sudo
#     ## On most platforms used, smartctl requires root access. Setting 'use_sudo'
#     ## to true will make use of sudo to run smartctl. Sudo must be configured to
#     ## allow the telegraf user to run smartctl without a password.
#     # use_sudo = false
#
#     ## Devices to include or exclude
#     ## By default, the plugin will use all devices found in the output of
#     ## `smartctl --scan-open`. Only one option is allowed at a time. If set, include
#     ## sets the specific devices to scan, while exclude omits specific devices.
#     # devices_include = []
#     # devices_exclude = []
#
#     ## Skip checking disks in specified power mode
#     ## Defaults to "standby" to not wake up disks that have stopped rotating.
#     ## For full details on the options here, see the --nocheck section in the
#     ## smartctl man page. Choose from:
#     ##   * never: always check the device
#     ##   * sleep: check the device unless it is in sleep mode
#     ##   * standby: check the device unless it is in sleep or standby mode
#     ##   * idle: check the device unless it is in sleep, standby, or idle mode
#     # nocheck = "standby"
#
#     ## Timeout for the cli command to complete
#     # timeout = "30s"


###############################################################################
#                            SERVICE INPUT PLUGINS                            #
###############################################################################


# # Read logging output from the Docker engine
# [[inputs.docker_log]]
#   ## Docker Endpoint
#   ##   To use TCP, set endpoint = "tcp://[ip]:[port]"
#   ##   To use environment variables (ie, docker-machine), set endpoint = "ENV"
#   # endpoint = "unix:///var/run/docker.sock"
#
#   ## When true, container logs are read from the beginning; otherwise reading
#   ## begins at the end of the log. If state-persistence is enabled for Telegraf,
#   ## the reading continues at the last previously processed timestamp.
#   # from_beginning = false
#
#   ## Timeout for Docker API calls.
#   # timeout = "5s"
#
#   ## Containers to include and exclude. Globs accepted.
#   ## Note that an empty array for both will include all containers
#   # container_name_include = []
#   # container_name_exclude = []
#
#   ## Container states to include and exclude. Globs accepted.
#   ## When empty only containers in the "running" state will be captured.
#   # container_state_include = []
#   # container_state_exclude = []
#
#   ## docker labels to include and exclude as tags.  Globs accepted.
#   ## Note that an empty array for both will include all labels as tags
#   # docker_label_include = []
#   # docker_label_exclude = []
#
#   ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars
#   source_tag = false
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false


# Generic socket listener capable of handling multiple socket types.
[[inputs.socket_listener]]
  ## URL to listen on
  service_address = "tcp://:2003"
  # service_address = "udp://:8094"

  ## Maximum number of concurrent connections (only available on stream sockets like TCP)
  ## Zero means unlimited.
  # max_connections = 0

  ## Read timeout (only available on stream sockets like TCP)
  ## Zero means unlimited.
  # read_timeout = "0s"

  ## Optional TLS configuration (only available on stream sockets like TCP)
  # tls_cert = "/etc/telegraf/cert.pem"
  # tls_key  = "/etc/telegraf/key.pem"
  ## Enables client authentication if set.
  # tls_allowed_cacerts = ["/etc/telegraf/clientca.pem"]

  ## Maximum socket buffer size (in bytes when no unit specified)
  ## For stream sockets, once the buffer fills up, the sender will start
  ## backing up. For datagram sockets, once the buffer fills up, metrics will
  ## start dropping. Defaults to the OS default.
  # read_buffer_size = "64KiB"

  ## Period between keep alive probes (only applies to TCP sockets)
  ## Zero disables keep alive probes. Defaults to the OS configuration.
  # keep_alive_period = "5m"

  ## Content encoding for message payloads
  ## Can be set to "gzip" for compressed payloads or "identity" for no encoding.
  # content_encoding = "identity"

  ## Maximum size of decoded packet (in bytes when no unit specified)
  # max_decompression_size = "500MB"

  ## Message splitting strategy and corresponding settings for stream sockets
  ## (tcp, tcp4, tcp6, unix or unixpacket). The setting is ignored for packet
  ## listeners such as udp.
  ## Available strategies are:
  ##   newline         -- split at newlines (default)
  ##   null            -- split at null bytes
  ##   delimiter       -- split at delimiter byte-sequence in hex-format
  ##                      given in `splitting_delimiter`
  ##   fixed length    -- split after number of bytes given in `splitting_length`
  ##   variable length -- split depending on length information received in the
  ##                      data. The length field information is specified in
  ##                      `splitting_length_field`.
  # splitting_strategy = "newline"

  ## Delimiter used to split received data to messages consumed by the parser.
  ## The delimiter is a hex byte-sequence marking the end of a message
  ## e.g. "0x0D0A", "x0d0a" or "0d0a" marks a Windows line-break (CR LF).
  ## The value is case-insensitive and can be specified with "0x" or "x" prefix
  ## or without.
  ## Note: This setting is only used for splitting_strategy = "delimiter".
  # splitting_delimiter = ""

  ## Fixed length of a message in bytes.
  ## Note: This setting is only used for splitting_strategy = "fixed length".
  # splitting_length = 0

  ## Specification of the length field contained in the data to split messages
  ## with variable length. The specification contains the following fields:
  ##  offset        -- start of length field in bytes from begin of data
  ##  bytes         -- length of length field in bytes
  ##  endianness    -- endianness of the value, either "be" for big endian or
  ##                   "le" for little endian
  ##  header_length -- total length of header to be skipped when passing
  ##                   data on to the parser. If zero (default), the header
  ##                   is passed on to the parser together with the message.
  ## Note: This setting is only used for splitting_strategy = "variable length".
  # splitting_length_field = {offset = 0, bytes = 0, endianness = "be", header_length = 0}

  ## Data format to consume.
  ## Each data format has its own unique set of configuration options, read
  ## more about them here:
  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
  data_format = "graphite"
